"""
ç”±å°ç‰›ç¿»è¯‘æä¾›çš„æ–‡æ¡£è§£æèƒ½åŠ›
æ”¯æŒè§£æPDFã€WORDã€EXCELã€PPTï¼Œç›´æ¥è¿”å›è§£æåçš„Markdownå†…å®¹
"""
import os
import tempfile
import threading
import time
import zipfile
from io import BytesIO
from pathlib import Path
from typing import Annotated, Dict
from uuid import uuid4

import gradio as gr
import requests
from fastapi import UploadFile
from mcp.server.fastmcp import FastMCP
from mcp.types import Field
from tqdm import tqdm
from download import download_file


document_cache: Dict[str, Dict] = {}


def generate_document_id() -> str:
    """ç”Ÿæˆå”¯ä¸€æ–‡æ¡£IDï¼ˆç”¨äºæ ‡è¯†ç¼“å­˜ä¸­çš„åˆ†æ®µæ•°æ®ï¼‰"""
    return str(uuid4())


# åˆå§‹åŒ– MCP æœåŠ¡å™¨
mcp = FastMCP("NiuTrans_Document_Parse")


class DocumentTransClient:
    """æ–‡æ¡£è½¬æ¢APIå®¢æˆ·ç«¯"""
    def __init__(self, base_url="http://your-api-domain.com"):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()

    def upload_file(self, file, **kwargs):
        """ä¸Šä¼ æ–‡ä»¶å¹¶å¤„ç†APIå“åº”"""
        url = f"{self.base_url}/documentConvert/documentConvertByFile"
        files = {'file': (kwargs.get('fileName'), file)}
        data = {**kwargs}

        try:
            resp = self.session.post(url, files=files, data=data)
            resp_json = resp.json()

            print(f"apiä¸Šä¼ è¿”å›å€¼: {resp_json}")

            # æ£€æŸ¥HTTPçŠ¶æ€ç 
            if resp.status_code != 200:
                error_msg = resp_json.get('msg', f"APIè¿”å›é”™è¯¯çŠ¶æ€ç : {resp.status_code}")
                raise Exception(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {error_msg}")

            # æ£€æŸ¥ä¸šåŠ¡é€»è¾‘é”™è¯¯
            code = resp_json.get('code', 200)
            if code != 200:
                error_msg = resp_json.get('msg', f"ä¸šåŠ¡é”™è¯¯ï¼Œé”™è¯¯ç : {code}")
                raise Exception(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {error_msg}")

            # æ£€æŸ¥dataæ˜¯å¦å­˜åœ¨
            if 'data' not in resp_json or resp_json['data'] is None:
                raise Exception("æ–‡ä»¶ä¸Šä¼ å¤±è´¥: APIè¿”å›æ•°æ®ä¸ºç©º")

            return resp_json['data']

        except Exception as e:
            # å¦‚æœæ˜¯å·²æ•è·çš„å¼‚å¸¸ï¼Œç›´æ¥é‡æ–°æŠ›å‡º
            if isinstance(e, Exception) and "æ–‡ä»¶ä¸Šä¼ å¤±è´¥" in str(e):
                raise

            # å¤„ç†å…¶ä»–å¼‚å¸¸
            error_msg = f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
            # å°è¯•ä»å“åº”ä¸­è·å–æ›´å¤šé”™è¯¯ä¿¡æ¯
            if 'resp_json' in locals():
                error_msg += f"ï¼ŒAPIå“åº”: {resp_json}"
            raise Exception(error_msg)

    def get_status(self, file_uuid):
        """æŸ¥è¯¢æ–‡ä»¶å¤„ç†çŠ¶æ€å¹¶å¤„ç†APIå“åº”"""
        url = f"{self.base_url}/documentConvert/getDocumentInfo"
        params = {'fileUuid': file_uuid}

        try:
            resp = self.session.get(url, params=params)
            resp_json = resp.json()

            print(f"çŠ¶æ€æŸ¥è¯¢è¿”å›å€¼: {resp_json}")

            # æ£€æŸ¥HTTPçŠ¶æ€ç 
            if resp.status_code != 200:
                error_msg = resp_json.get('msg', f"APIè¿”å›é”™è¯¯çŠ¶æ€ç : {resp.status_code}")
                raise Exception(f"çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {error_msg}")

            # æ£€æŸ¥ä¸šåŠ¡é€»è¾‘é”™è¯¯
            code = resp_json.get('code', 200)
            if code != 200:
                error_msg = resp_json.get('msg', f"ä¸šåŠ¡é”™è¯¯ï¼Œé”™è¯¯ç : {code}")
                raise Exception(f"çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {error_msg}")

            # æ£€æŸ¥dataæ˜¯å¦å­˜åœ¨
            if 'data' not in resp_json or resp_json['data'] is None:
                raise Exception("çŠ¶æ€æŸ¥è¯¢å¤±è´¥: APIè¿”å›æ•°æ®ä¸ºç©º")

            return resp_json['data']

        except Exception as e:
            # å¦‚æœæ˜¯å·²æ•è·çš„å¼‚å¸¸ï¼Œç›´æ¥é‡æ–°æŠ›å‡º
            if isinstance(e, Exception) and "çŠ¶æ€æŸ¥è¯¢å¤±è´¥" in str(e):
                raise

            # å¤„ç†å…¶ä»–å¼‚å¸¸
            error_msg = f"çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {str(e)}"
            # å°è¯•ä»å“åº”ä¸­è·å–æ›´å¤šé”™è¯¯ä¿¡æ¯
            if 'resp_json' in locals():
                error_msg += f"ï¼ŒAPIå“åº”: {resp_json}"
            raise Exception(error_msg)

    def download_file(self, file_uuid, save_path, file_type=3):
        url = f"{self.base_url}/documentConvert/downloadFile"
        params = {'fileUuid': file_uuid, 'type': file_type}
        try:
            with self.session.get(url, params=params, stream=True) as resp:
                resp.raise_for_status()
                total_size = int(resp.headers.get('content-length', 0))
                with open(save_path, 'wb') as f, tqdm(
                        desc="ä¸‹è½½è§£æç»“æœ",
                        total=total_size,
                        unit='B',
                        unit_scale=True,
                        unit_divisor=1024
                ) as bar:
                    for chunk in resp.iter_content(chunk_size=8192):
                        f.write(chunk)
                        bar.update(len(chunk))
            return save_path
        except Exception as e:
            raise Exception(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}")

    def wait_for_completion(self, file_uuid, interval=2, timeout=3600) -> dict:
        start_time = time.time()
        last_progress = 0
        with tqdm(desc="æ–‡æ¡£è§£æè¿›åº¦", unit="%") as pbar:
            while True:
                status_data = self.get_status(file_uuid)
                current_status = status_data['fileStatus']
                current_progress = int(status_data['progress'] * 100)
                if current_progress > last_progress:
                    pbar.update(current_progress - last_progress)
                    last_progress = current_progress
                if current_status == 202:
                    pbar.update(100 - last_progress)
                    return status_data
                elif current_status >= 204:
                    error_msg = status_data.get('transFailureCause', 'æœªçŸ¥é”™è¯¯')
                    raise Exception(f"è§£æå¤±è´¥: {error_msg}")
                elif time.time() - start_time > timeout:
                    raise TimeoutError(f"è§£æè¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰")
                time.sleep(interval)


def extract_text_from_zip(zip_path: str) -> str:
    """ä»ZIPæ–‡ä»¶ä¸­æå–Markdownæ–‡æœ¬å†…å®¹"""
    text_content = ""
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        for file_info in zip_ref.infolist():
            # ä¼˜å…ˆæå–Markdownæ–‡ä»¶ï¼ˆåŸè§£æé€»è¾‘ç”Ÿæˆçš„ä¸»è¦æ–‡æœ¬æ ¼å¼ï¼‰
            if file_info.filename.lower().endswith('.md'):
                with zip_ref.open(file_info) as f:
                    text_content += f.read().decode('utf-8', errors='ignore') + "\n\n"
        # è‹¥æ²¡æœ‰Markdownï¼Œå°è¯•æå–TXT
        if not text_content:
            for file_info in zip_ref.infolist():
                if file_info.filename.lower().endswith('.txt'):
                    with zip_ref.open(file_info) as f:
                        text_content += f.read().decode('utf-8', errors='ignore') + "\n\n"
    if not text_content:
        raise Exception("ZIPæ–‡ä»¶ä¸­æœªæ‰¾åˆ°å¯è§£æçš„æ–‡æœ¬å†…å®¹ï¼ˆæ— MD/TXTæ–‡ä»¶ï¼‰")
    return text_content.strip()


def call_document_convert_api(file) -> str:
    """è°ƒç”¨æ–‡æ¡£è½¬æ¢APIè·å–è§£æåçš„æ–‡æœ¬ï¼ˆä¸»è¦æ˜¯Markdownï¼‰"""
    client = DocumentTransClient(base_url="http://82.156.10.7:10064/")
    try:
        file_uuid = client.upload_file(
            file=file.file,
            toFileSuffix="markdown",  # æ˜ç¡®è¦æ±‚è¿”å›Markdownæ ¼å¼
            fileName=file.filename
        )
        print(f"æ–‡æ¡£è§£æä»»åŠ¡æäº¤æˆåŠŸï¼Œä»»åŠ¡ID: {file_uuid}")
        status_data = client.wait_for_completion(file_uuid)
        with tempfile.TemporaryDirectory() as temp_dir:
            zip_save_path = os.path.join(temp_dir, f"parsed_{file_uuid}.zip")
            client.download_file(file_uuid, zip_save_path)
            text_content = extract_text_from_zip(zip_save_path)
        return text_content
    except Exception as e:
        raise Exception(
            f"è§£æå¤±è´¥ï¼šå¯èƒ½æ˜¯æ–‡ä»¶æ ¼å¼é”™è¯¯æˆ–APIè¿æ¥é—®é¢˜ã€‚"
            f"åŸå§‹é”™è¯¯ï¼š{str(e)}"
        )


def preprocess_raw_text(raw_text: str) -> str:
    """ç®€å•é¢„å¤„ç†Markdownæ–‡æœ¬ï¼ˆå»é™¤ä¹±ç å’Œå¤šä½™ç©ºè¡Œï¼‰"""
    lines = [line.strip() for line in raw_text.splitlines() if line.strip()]
    cleaned_text = "\n".join(lines)
    cleaned_text = cleaned_text.replace("\\u0000", "").replace("ï¿½", "").replace("\r", "")
    return cleaned_text


def split_markdown_into_chunks(markdown_text: str, chunk_size=3000) -> list:
    """
    å°†Markdownæ–‡æœ¬åˆ†æ®µ
    - ä¼˜å…ˆæŒ‰ä¸€çº§æ ‡é¢˜ï¼ˆ#ï¼‰åˆ†å‰²ï¼ˆä¿æŒç« èŠ‚å®Œæ•´æ€§ï¼‰
    - æ— æ˜æ˜¾æ ‡é¢˜æ—¶æŒ‰å›ºå®šå­—ç¬¦æ•°åˆ†å‰²ï¼ˆé¿å…æˆªæ–­å¥å­ï¼‰
    """
    chunks = []
    lines = markdown_text.splitlines()
    current_chunk = []
    current_length = 0

    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸€çº§æ ‡é¢˜ï¼ˆ# å¼€å¤´ï¼‰
    has_level1_heading = any(line.strip().startswith("# ") for line in lines)

    if has_level1_heading:
        # æŒ‰ä¸€çº§æ ‡é¢˜åˆ†å‰²ï¼ˆæ¯ä¸ªç« èŠ‚ä½œä¸ºä¸€æ®µï¼‰
        for line in lines:
            stripped_line = line.strip()
            # é‡åˆ°æ–°çš„ä¸€çº§æ ‡é¢˜ï¼Œä¸”å½“å‰chunkä¸ä¸ºç©ºï¼Œä¿å­˜å½“å‰chunk
            if stripped_line.startswith("# ") and current_chunk:
                chunks.append("\n".join(current_chunk))
                current_chunk = [line]  # æ–°chunkä»¥å½“å‰æ ‡é¢˜å¼€å§‹
            else:
                current_chunk.append(line)
        # ä¿å­˜æœ€åä¸€ä¸ªchunk
        if current_chunk:
            chunks.append("\n".join(current_chunk))
    else:
        # æ— æ˜æ˜¾æ ‡é¢˜ï¼ŒæŒ‰å›ºå®šé•¿åº¦åˆ†å‰²ï¼ˆå°½é‡åœ¨æ¢è¡Œ/å¥å·å¤„åˆ†å‰²ï¼‰
        for line in lines:
            line_length = len(line)
            # å¦‚æœåŠ å…¥å½“å‰è¡Œè¶…è¿‡chunk_sizeï¼Œå…ˆä¿å­˜å½“å‰chunk
            if current_length + line_length > chunk_size and current_chunk:
                # å°è¯•åœ¨å¥å°¾åˆ†å‰²
                chunk_str = "\n".join(current_chunk)
                last_period = chunk_str.rfind(".")
                last_newline = chunk_str.rfind("\n")
                split_pos = max(last_period, last_newline) if (last_period != -1 or last_newline != -1) else len(chunk_str)
                # åˆ†å‰²å¹¶ä¿å­˜
                chunks.append(chunk_str[:split_pos+1].rstrip())
                # å‰©ä½™éƒ¨åˆ†ä½œä¸ºæ–°chunkçš„å¼€å§‹
                remaining = chunk_str[split_pos+1:].lstrip()
                current_chunk = [remaining] if remaining else []
                current_length = len(remaining)
            # åŠ å…¥å½“å‰è¡Œ
            current_chunk.append(line)
            current_length += line_length
        # ä¿å­˜æœ€åä¸€ä¸ªchunk
        if current_chunk:
            chunks.append("\n".join(current_chunk))

    # è¿‡æ»¤ç©ºåˆ†æ®µ
    return [chunk for chunk in chunks if chunk.strip()]


class UploadFileWrapper:
    """æ¨¡æ‹ŸFastAPIçš„UploadFileç±»"""

    def __init__(self, file_path: str):
        self.file_path = file_path
        self.filename = os.path.basename(file_path)
        self.file = open(file_path, 'rb')

    def close(self):
        """å…³é—­æ–‡ä»¶"""
        if hasattr(self, 'file') and not self.file.closed:
            self.file.close()


# ------------------------------
# MCP å·¥å…·
# ------------------------------
@mcp.tool()
def parse_document(
        file_url: Annotated[
            str,
            Field(
                description="""URLï¼Œæ”¯æŒä»¥ä¸‹æ ¼å¼:
            - å•ä¸ªURL: "https://example.com/document.pdf"
            (æ”¯æŒpdfã€pptã€pptxã€docã€docxã€xlsã€xlsx)"""
            ),
        ]
) -> Dict[str, str]:
    """
    ç»Ÿä¸€æ¥å£ï¼Œå°†æ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚æ”¯æŒURLã€‚

    - å°†http/httpså¼€å¤´çš„è·¯å¾„ä¸‹è½½æ–‡ä»¶å¹¶å¤„ç†

    å¤„ç†å®Œæˆåï¼Œä¼šè¿”å›æˆåŠŸçš„æ–‡ä»¶idå’Œåˆ†æ®µæ•°æ ¹æ®æ–‡ä»¶idå’Œåˆ†æ®µæ•°è°ƒç”¨get_document_chunk()ã€‚

    Args:
        file_url (str): å¯ä¸‹è½½çš„æ–‡ä»¶url,ç¤ºä¾‹:"https://example.com/document.pdf"ã€‚

    è¿”å›:
        æˆåŠŸ: {"status": "success", "document_id": "æ–‡ä»¶id""total_chunks": æ€»åˆ†æ®µæ•°,"filename": æ–‡ä»¶å,}
        å¤±è´¥: {"status": "error", "error": "é”™è¯¯ä¿¡æ¯"}
    """
    fake_file = None
    try:
        if not file_url:
            return {"status": "error", "error": "æœªæä¾›æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„æˆ–URL"}

        if not file_url.lower().startswith(("http://", "https://")):
            return {"status": "error", "error": "æœªæä¾›æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„æˆ–URL"}

        # ä¸‹è½½æ–‡ä»¶
        file_path = download_file(url=file_url, save_directory="uploadFile")

        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        file_suffix = Path(file_path).suffix.lower()
        supported_types = [".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx"]
        if file_suffix not in supported_types:
            return {"status": "error", "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ã€‚æ”¯æŒçš„ç±»å‹: {', '.join(supported_types)}"}

        # åˆ›å»ºæ¨¡æ‹Ÿçš„UploadFileå¯¹è±¡
        fake_file = UploadFileWrapper(file_path)

        try:
            # è°ƒç”¨æ–‡æ¡£è½¬æ¢API
            text_content = call_document_convert_api(fake_file)

            # é¢„å¤„ç†æ–‡æœ¬
            cleaned_content = preprocess_raw_text(text_content)

            # æ–‡æœ¬åˆ†å—
            chunks = split_markdown_into_chunks(cleaned_content)

            # ç”Ÿæˆæ–‡æ¡£ID
            doc_id = generate_document_id()
            print(f"è§£æç»“æœid: {doc_id}")

            # å­˜å…¥ç¼“å­˜
            document_cache[doc_id] = {
                "filename": fake_file.filename,
                "chunks": chunks,
                "total_chunks": len(chunks)
            }
            return {
                "document_id": doc_id,
                "total_chunks": str(len(chunks)),
                "filename": fake_file.filename,
                "status": "success"
            }

        except Exception as e:
            return {"status": "error", "error": f"è§£æå¤±è´¥ï¼š{str(e)}"}

    except Exception as e:
        return {"status": "error", "error": f"å¤„ç†å¤±è´¥ï¼š{str(e)}"}

    finally:
        # ç¡®ä¿æ–‡ä»¶è¢«å…³é—­å’Œåˆ é™¤
        if fake_file:
            fake_file.close()

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if 'file_path' in locals() and os.path.exists(file_path):
            try:
                os.remove(file_path)
                print(f"ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤: {file_path}")
            except Exception as e:
                print(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")


@mcp.tool()
def get_document_chunk(
        document_id: Annotated[
            str,
            Field(description="ç”±parse_documentè¿”å›çš„æ–‡æ¡£ID")
        ],
        chunk_index: Annotated[
            int,
            Field(description="è¦è·å–çš„æ®µè½ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼Œå¦‚0è¡¨ç¤ºç¬¬1æ®µï¼Œ1è¡¨ç¤ºç¬¬2æ®µ...ï¼‰")
        ]
) -> Dict[str, str]:
    """
    æ ¹æ®æ–‡æ¡£IDå’Œç´¢å¼•ï¼Œè¿”å›æŒ‡å®šçš„åˆ†æ®µå†…å®¹ï¼ˆå•æ¬¡è¿”å›ä¸€æ®µï¼Œé¿å…å†…å®¹è¿‡é•¿ï¼‰

    æ ¹æ®æ–‡æ¡£IDå’Œåˆ†æ®µç´¢å¼•è·å–å…·ä½“çš„æ–‡æ¡£æ®µè½å†…å®¹ã€‚éœ€è¦å…ˆè°ƒç”¨parse_document()è·å–document_idã€‚

    è¿”å›:
        æˆåŠŸ: {
            "document_id": "æ–‡æ¡£ID",
            "current_chunk": å½“å‰æ®µå·ï¼ˆä»1å¼€å§‹ï¼‰,
            "total_chunks": æ–‡æ¡£æ€»åˆ†æ®µæ•°,
            "content": "å½“å‰æ®µçš„Markdownæ ¼å¼å†…å®¹"
        }
        å¤±è´¥: æŠ›å‡ºå¼‚å¸¸ï¼ŒåŒ…å«é”™è¯¯ä¿¡æ¯
    """
    if document_id not in document_cache:
        raise ValueError(f"æ— æ•ˆçš„document_idï¼š{document_id}ï¼ˆæ–‡æ¡£æœªè§£ææˆ–å·²è¿‡æœŸï¼‰")

    doc_data = document_cache[document_id]
    total = doc_data["total_chunks"]
    if chunk_index < 0 or chunk_index >= total:
        raise IndexError(f"æ®µè½ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼ˆæ€»æ®µæ•°ï¼š{total}ï¼Œè¯·ä¼ å…¥0~{total - 1}ï¼‰")

    return {
        "document_id": document_id,
        "current_chunk": chunk_index + 1,  # æ˜¾ç¤ºç¬¬xæ®µï¼ˆäººç±»å¯è¯»ï¼‰
        "total_chunks": total,
        "content": doc_data["chunks"][chunk_index]  # ä»…è¿”å›å½“å‰æ®µå†…å®¹
    }


@mcp.resource("document://supported-types")
def get_supported_file_types() -> Dict[str, list]:
    return {
        "supported_types": [
            {"format": "PDF", "extensions": [".pdf"], "mime_type": "application/pdf"},
            {"format": "Word", "extensions": [".doc", ".docx"],
             "mime_type": ["application/msword", "application/vnd.openxmlformats-officedocument.wordprocessingml.document"]},
            {"format": "Excel", "extensions": [".xls", ".xlsx"],
             "mime_type": ["application/vnd.ms-excel", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"]},
            {"format": "PPT", "extensions": [".ppt", ".pptx"],
             "mime_type": ["application/vnd.ms-powerpoint", "application/vnd.openxmlformats-officedocument.presentationml.presentation"]}
        ],
        "description": "æ”¯æŒè§£ææ–‡æ¡£å¹¶è¿”å›æå–çš„Markdownæ ¼å¼å†…å®¹"
    }


# ------------------------------
# Gradio ç•Œé¢ï¼ˆç®€åŒ–ï¼Œä»…å±•ç¤ºè§£æåçš„Markdownï¼‰
# ------------------------------
def gradio_parse_from_url(url: str) -> str:
    """é€šè¿‡URLè§£ææ–‡æ¡£"""
    if not url:
        return "è¯·è¾“å…¥æœ‰æ•ˆçš„URL"

    try:
        result = parse_document(file_url=url)

        if result["status"] == "success":
            return f"""è§£ææˆåŠŸï¼
æ–‡æ¡£IDï¼š{result['document_id']}
æ–‡ä»¶åï¼š{result['filename']}
æ€»æ®µæ•°ï¼š{result['total_chunks']}æ®µ

å¯åœ¨ä¸‹æ–¹è¾“å…¥æ–‡æ¡£IDå’Œæ®µç´¢å¼•è·å–å…·ä½“å†…å®¹"""
        else:
            return f"è§£æå¤±è´¥ï¼š{result['error']}"

    except Exception as e:
        return f"è§£æå¤±è´¥ï¼š{str(e)}"


def gradio_get_chunk(doc_id: str, chunk_idx: int) -> str:
    """æ ¹æ®IDå’Œç´¢å¼•è·å–åˆ†æ®µå†…å®¹"""
    if not doc_id:
        return "è¯·è¾“å…¥æ–‡æ¡£ID"

    if chunk_idx is None:
        return "è¯·è¾“å…¥æ®µç´¢å¼•"

    try:
        result = get_document_chunk(document_id=doc_id, chunk_index=chunk_idx)
        return f"""## ç¬¬{result['current_chunk']}/{result['total_chunks']}æ®µ

{result['content']}"""
    except ValueError as e:
        return f"é”™è¯¯ï¼š{str(e)}"
    except IndexError as e:
        return f"é”™è¯¯ï¼š{str(e)}"
    except Exception as e:
        return f"è·å–å¤±è´¥ï¼š{str(e)}"


def create_gradio_interface():
    with gr.Blocks(title="æ–‡æ¡£åˆ†æ®µè§£æå·¥å…·ï¼ˆURLç‰ˆï¼‰") as demo:
        gr.Markdown("# ğŸ“„ æ–‡æ¡£åˆ†æ®µè§£æå·¥å…·")
        gr.Markdown("é€šè¿‡URLè§£ææ–‡æ¡£ï¼Œå¹¶æ”¯æŒåˆ†æ®µè·å–å†…å®¹")

        # æ˜¾ç¤ºæ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_info = get_supported_file_types()
        supported_formats = ", ".join([f"{item['format']} ({', '.join(item['extensions'])})"
                                       for item in supported_info["supported_types"]])
        gr.Markdown(f"**æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š** {supported_formats}")

        with gr.Row():
            # ç¬¬ä¸€æ­¥ï¼šé€šè¿‡URLè§£ææ–‡æ¡£
            with gr.Column(scale=1):
                gr.Markdown("### ç¬¬ä¸€æ­¥ï¼šè¾“å…¥æ–‡æ¡£URL")
                url_input = gr.Textbox(
                    label="æ–‡æ¡£URL",
                    placeholder="https://example.com/document.pdf",
                    lines=2
                )
                parse_btn = gr.Button("è§£ææ–‡æ¡£", variant="primary")
                parse_result = gr.Textbox(label="è§£æç»“æœ", lines=6)

            # ç¬¬äºŒæ­¥ï¼šè·å–æŒ‡å®šåˆ†æ®µ
            with gr.Column(scale=1):
                gr.Markdown("### ç¬¬äºŒæ­¥ï¼šè·å–åˆ†æ®µå†…å®¹")
                doc_id_input = gr.Textbox(
                    label="æ–‡æ¡£ID",
                    placeholder="ä»è§£æç»“æœè·å–",
                    lines=1
                )

                # æ®µç´¢å¼•è¾“å…¥
                chunk_idx_input = gr.Number(
                    label="æ®µç´¢å¼•",
                    value=0,
                    step=1,
                    minimum=0
                )

                get_chunk_btn = gr.Button("è·å–è¯¥æ®µå†…å®¹", variant="secondary")
                chunk_result = gr.Markdown(label="åˆ†æ®µå†…å®¹", value="è¯·å…ˆè§£ææ–‡æ¡£è·å–ID")

        # ç»‘å®šè§£ææŒ‰é’®
        parse_btn.click(
            fn=gradio_parse_from_url,
            inputs=url_input,
            outputs=parse_result
        )

        # ç»‘å®šè·å–åˆ†æ®µæŒ‰é’®
        get_chunk_btn.click(
            fn=gradio_get_chunk,
            inputs=[doc_id_input, chunk_idx_input],
            outputs=chunk_result
        )

        # æ·»åŠ ç¤ºä¾‹
        gr.Examples(
            examples=[
                ["https://example.com/sample.pdf"],
                ["https://example.com/report.docx"],
                ["https://example.com/presentation.pptx"]
            ],
            inputs=url_input,
            label="ç¤ºä¾‹URL"
        )

    return demo


# ------------------------------
# å¯åŠ¨æœåŠ¡
# ------------------------------
def run_mcp_server():
    mcp.run(transport="streamable-http")


def main():
    mcp_thread = threading.Thread(target=run_mcp_server, daemon=True)
    mcp_thread.start()
    time.sleep(1)  # ç­‰å¾…MCPæœåŠ¡å¯åŠ¨
    demo = create_gradio_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        mcp_server=True
    )


if __name__ == "__main__":
    main()